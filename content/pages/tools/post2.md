Title: Improving evidence search
Date: 2021-07-15 15:30
Summary: How many people have built lots of tools to make **evidence search** easier and faster. What good ideas they already had. How tools still fall short of what clinicians and reviewers need. How developing tools is often hard, as previous work cannot be re-used. How we do not know how design decisions affect search outcomes.
Show_on_main: True
bibliography: ../articles.bib
<img alt="Tool icon" src="/images/hammer-line.png" style="max-width:25%">

# Summary
How many people have built lots of tools to make **evidence search** easier and faster. What good ideas they already had. How tools still fall short of what clinicians and reviewers need. How developing tools is often hard, as previous work cannot be re-used. How we do not know how design decisions affect search outcomes.

# Evidence search engines
How can clinicians answer a research question with relevant primary literature? They could run a search via PubMed, the most famous and widely used search engine for biomedical questions that is operated by the National Library of Medicine (NLM). Thanks to its detailed human-curated indexing and article classification by study type, PubMed makes it possible to run fine-grained searches with high precision. PubMed has become an institutional standard, being mandated as one of the primary tools for running literature searches during the creation of systematic reviews. However, it is also somewhat unwieldy to use and necessarily less innovative than other vendors who offer a less well established product. For this reason, over the years, there have been countless efforts to build an evidence search engine that goes beyond what PubMed offers, adding functionalities such as automatic generation of indexing terms to reduce the time until new articles become indexed or machine-aided aggregation functions that do more than just retrieving abstracts of relevant articles. A 2011 survey[@Lu2011] of 28 such search engines distinguishes four areas which they aim to improve, namely the ranking of search results, allowing a clustering of results by topic, extracting and displaying semantics and improving the search interface. Many of them have since been deprecated, but features and ideas first promoted by them have occasionally been added into PubMed itself. In the 10 years since the survey came out, the production of supposedly innovative clinical search engine prototypes has only accelerated. Especially during the Covid-19 pandemic, teams all across the world raced to build search tools for helping clinicians find answers in the similarly rapidly expanding literature on the disease. A 2020 survey[@Wang2020a] identifies 30 search tools created in response to the pandemic, most of which indexed the same corpus and differ in implementation and design, with innovative features such as automatic summarization being mixed in in some instances.

Apparently, searching for the right search tool is its own problem these days if one aims to maximize one's use of innovative technology. However, unfortunately proper evaluation of the utility of some of the proposed changes is very widespread. Most of the evaluation done up this point follows classical information retrieval paradigms, mostly trying to optimize the search engine's ability to rank documents deemed relevant to a given query higher than irrelevant ones, but failing to quantize the actual effects that using the search engine has on clinical practice. Recent research[@Vegt2021] demonstrated that higher relevance scores obtained by intensive tweaking and optimization on behalf of the IR researchers did not help one search engine to outperform a simple baseline when it comes to actually improving how clinicians put to practice the results of their search. In other words, even though one search engine was ranking the most relevant documents higher than the other ones, the conclusions that the clinicians derived had the same chance of being unfounded by the evidence. The authors find that beyond retrieving relevant documents, helping users understand the documents they're reading in context is a crucial ability of search engines that is overlooked in the current evaluation protocol.

# Evaluating current tools
To help the inform the design of clinical search engines, we delineated a number of criteria that clinicians value when performing a critical appraisal of the literature. We also evaluated a small set of innovative general-purpose evidence search engines based on them.



_Trialstreamer and the whole list of products we discovered. More controlled search queries thanks to PICO indexing. Filtering by resource type (e.g. clinical trial)._

_In the search area, we can discuss what J and his colleague find to be useful features in their evaluation. We find that many useful functionalities are already there, but isolated and spread across products, yielding a frustrating user experience that prevents further up-take. We also find a distinct potential for re-using existing work that is currently not leveraged. A more detailed comparison will be linked to via [separate page](engine-comparison.html)._

# Building tools is hard
To create a useful tool, there is usually a lot of up-front work required, ingesting huge and ever-growing corpora of research documents, pre-processing to apply certain techniques, such as named entity recognition or article classification. A tool then needs to be maintained, which is costly and requires manual effort. Frequently, tools are created as offshoots from temporally limited research projects, so once the funding runs out, support does too. In a risk-averse sector such as health care, this tenuous situation further hampers up-take, as clinicians are wary of investing the time to learn to use a new tool that might be gone the next year. 