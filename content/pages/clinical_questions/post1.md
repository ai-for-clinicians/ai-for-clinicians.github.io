Title: Clinical questions
Date: 2021-07-15 15:30
Summary: How clinicians currently search relevant clinical evidence to help their patients. Why systematic reviews and guidelines are really important to them, but frequently outdated. How creating these documents is a meticulous, time-consuming process and why they're outdated when they come out already. How the **evidence ecosystem** is not meeting the needs of stakeholders: Precise, relevant information that helps my particular patient.
Show_on_main: True
bibliography: ../articles.bib
<img alt="Search icon" src="/images/search-line.png" style="max-width:25%">

# Summary
How clinicians currently search relevant clinical evidence to help their patients. Why systematic reviews and guidelines are really important to them, but frequently outdated. How creating these documents is a meticulous, time-consuming process and why they're outdated when they come out already. How the **evidence ecosystem** is not meeting the needs of stakeholders: Precise, relevant information that helps my particular patient.

# What answers clinicians need
Clinicians work every day to help their patients get better. However, as our collective scientific and medical understanding of the world has been ever expanding, it is impossible for them to memorize all the relevant facts once during their training. While training requirements during their career as well as conventions and trade publications can help them to get to know new relevant facts, there is also a substantial need for them to engage in active search. When facing a new patient, they need to find relevant information to guide clinical decisions, such as what treatment options to consider. According to Hersh[@Hersh2020], one can distinguish between background versus foreground questions, with the latter being about a particular patient. Moreover, information needs can be unrecognized (when clinicians do not realize that they should search) or unmet (for example, when clinicians don't search because they don't think the information is available). To successfully treat a patient based on the best available evidence, clinicians need the awareness that they need to perform search, as well as the confidence in the tools that enable them to do so. 

# What resources are available
There are many resources that clinicians can turn to in order to find answers. Generally, they can be placed in a hierarchy like in the 4S model by Haynes[@Haynes2001].
<img alt="4S model according to Haynes" src="https://ebm.bmj.com/content/ebmed/6/2/36/F1.medium.gif" style="padding-top:1em">
At the bottom, studies provide a fast, uncorrected stream of new findings. Always on the edge of research, their results are to be considered as tentative, as spectacular initial findings often turn out to be flukes. One layer up, syntheses provide an integrated view over a longer period, combining and statistically aggregating findings from multiple studies. While also occasionally created by individual teams of authors, these analyses are now often produced under the auspices of organizations such as the Cochrane Collaboration, which vouch to maintain and update them as new study findings accumulate. They also often run to many pages and are dense in content, providing the incentive for the creation of synopses, such as the evidence abstracts distributed in journals or articles in specialist sources such as UpToDate or Dynamed. The latter are commercial vendors who sell more digestible synopses to clinicians and hospitals. Finally, at the uppermost layer of the hierarchy, we find systems. Here, the decision the clinician faces is answered automatically, e.g. by means of an algorithm whose inputs are clinical parameters and whose outputs guide or even constrain the actions of the clinicians. These systems are built atop of accumulated research findings, as the intention of their designers is to deeply integrate these findings into clinical practice, essentially not even requiring clinicians' conscious awareness.

Haynes' model is clearly prescriptive, as the author explicitly encourages clinicians to start their search at the top, rather than directly consulting the primary literature situated at the bottom. However, depending on the concrete question, the upper two layers will not provide sufficient answers, especially when the merits of a new treatment are to be evaluated. Moreover, even when answers are available at the uppermost layers, they can sometimes be outdated or biased, so clinicians will still often opt to conduct their own critical appraisal of syntheses and primary studies.

# The evidence ecosystem
The production and synthesis of clinical is a complex activity that relies on the collaboration of many people. Primary researchers design and run studies and publish their findings. Other researchers analyze data from multiple studies to produce reviews, which are then in turn synthesized again to yield artifacts such as guidelines and automated decision support systems. These then drive clinical practice, giving rise to new questions to be investigated by new trials. Researchers[@Boutron2020] have coined the term "evidence ecosystem" to describe this complex societal process. Their model is both descriptive and prescriptive, as they point out that while the looping flow of evidence already exists in the real world, there are also a number of problems that impede the flow of information and the improvement of clinical practice. 

<img alt="Evidence ecosystem" src="http://newwindow.no/magicproject/wp-content/uploads/2016/03/evidence-ecosystem.png" style="padding-top:1em">

Specifically, in their sometimes scathing assessment of the current situation, the authors lament duplication and gaps in what reviews cover, insufficient tracking of adverse events, long publication times that mean reviews are out of date when they come out, flawed search methods that mean many relevant papers go unnoticed and generally incomplete reporting, with a lack of best practices for sharing review data in interoperable formats. In general, they point out that the enterprise is poorly suited to meet the need of stakeholders, as reviews are frequently produced in a one-shot process with no perspective for future updates, their production is lengthy and expensive and they are often not sufficiently targeted to what clinicians really want to know about. In their view, clinicians don't only want a comparison of a particular treatment to placebo, but rather need a big picture comparison that contains all the available treatment options and evaluates them across many different outcomes to allow a full benefit-harm analysis. Based on promises about the age of personalized medicine being imminent, patients expect these insights to be targeted to the personal condition and circumstances. There is also more data than ever being produced, with many trials now releasing individual participant data that in theory allows much more fine-grained analysis if put to proper use.

# Meeting the needs of stakeholders
In the status quo, clinicians frequently don't search, because they don't think an answer even exists. Crucial resources are outdated because their creation is too complicated. Many questions in fact don't have a review that covers them. Reviews are often duplicated and sometimes of poor quality. These symptoms of the *evidence-to-practice gap* have been known and lamented for years, and intelligent technology has often been hailed as a possible solution. In our view, there are two ways to leverage it to help clinicians get relevant answers faster:

In one, which we will refer to as the *retrieval* option, tools are introduced that make it possible to side-step evidence synthesis, for example by allowing clinicians to stay informed on relevant primary literature directly or by offering them automatically generated syntheses of the literature with no need for human involvement. The underlying assumption is that clinicians do not wait for secondary resources to be updated, and instead set out to find out relevant evidence, including primary research, to inform their decisions. This approach is portrayed in the [second post](improving-evidence-search.html), where we compare different existing products that claim to make the evidence search experience better for clinicians.

In the other, which we will refer to as the *reinvention* option, technology facilitates the development of a digital infrastructure for an evidence ecosystem, allowing more and faster data sharing, such that in the future, reviews and guidelines will be updated faster, and clinicians will be informed faster. This option is more idealistic and offers less of a quick fix, but holds the promise of curing the disease, which is improper collaboration and data sharing, rather than just improving the symptoms for some doctors who are savvy enough to take the matter into their own hands. [Our third post] shows some existing efforts in this area. Finally, the [last post](a-vision-for-the-future.html) presents our own vision to change the evidence ecosystem for the better.
